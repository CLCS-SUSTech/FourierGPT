# FourierGPT -- A Psycholinguistics-inspired method for text detection

This repository contains code for the EMNLP 2024 paper **"Detecting Subtle Differences between Human and Model Languages Using Spectrum of Relative Likelihood"** ([arxiv](https://arxiv.org/abs/2406.19874)).

## What FourierGPT *is* and *is not*
FourierGPT is a method for detecting machine generated texts from human ones. For example, the following two pieces of text are responses to the same question, with one generated by GPT-4 and the other by human, respectively:

**Question**
```text
Question: Is an advance care planning model feasible in community palliative care?
```

**GPT-4's response**
```text
Answer: Yes, an advance care planning model is feasible in community palliative care. ...
```

**Human's response**
```text
Answer: An advance care planning model is feasible for community palliative care services. ...
```

While the two responses look very similar and convey the same meaning as well, there are still some subtle differences, such as the omitted *"Yes"* and the additional *"services"* in the human response.

Can we quantify these subtle differences? 

Our method applies Fourier transform to the relative likelihood scores of texts, which results in a spectrum view that can tell apart model and human responses: as shown in the figure below, GPT-4 has higher power magnitudes (on average) than human near the low frequency area of the spectrum.

![spectrum](images/spectrum_tiny.png)

FourierGPT is **NOT** a new language model based on GPT. The naming is inspired by the recently proposed zero-shot text detection methods [DetectGPT](https://arxiv.org/abs/2301.11305) and [Fast-DetectGPT](https://arxiv.org/abs/2310.05130).

Spectrum captures the how likelihood (surprisal, information density etc.) of language changes with time, which is related to psycholinguistics theories such as uniform information density (UID) [(see Meister et al. for a recent review)](https://aclanthology.org/2021.emnlp-main.74.pdf)

## Detection performance
FourierGPT works best on short text, such as the [PubMedQA dataset](https://aclanthology.org/D19-1259), and outperforms the SOTA. Part of the detection accuraries are as follows:

|Dataset|Gen. model|FourierGPT|Fast-DetectGPT|
|---|---|---|---|
|PubMed| GPT-4  | **0.9133** | 0.8503 |
|PubMed| GPT-3.5 | **0.9467** | 0.9021 |

More details reported in the [paper]().

## Procedure
The general procedure of FourierGPT is shown in the following figure:

![procedure](images/procedure.png)

which roughly breaks into three steps:

### 1. Estimate likelihood scores
To estimate likelihood scores with GPT-2 models:

```bash
python run_nll.py -i INPUT_TEXT_FILE -o OUTPUT_NLL_FILE --model_path MODEL_PATH
```

- It produces likelihood scores in `OUTPUT_NLL_FILE` (NLL for negative log likelihood), which is estimated by the model in `MODEL_PATH`.
- You may specify `--model_path` to the path of your locally saved GPT-2 models

If you wish to use a larger model, for instance, mistral-7b or llama-7b, then you need to specify `--model_path` with the path to your local model.

In the paper's experiment, [Mistral-7B-v0.1](https://huggingface.co/mistralai/Mistral-7B-v0.1) was used as the custom model. 

```bash
python run_nll.py -i INPUT_TEXT_FILE -o OUTPUT_NLL_FILE --config CONFIG_PATH
```

The resulting *raw* likelihood data can be used at **Step 2** to produce *normalized* likelihood scores and the correponding spectrum data, or can be directly used for the supervised learning-based classifier at **Step 3.1**.

### 2. Normalize likelihood and Fourier transform

Then, compute the normalized likelihood scores and then apply Fourier transform to obtain the spectrum data: 

```bash
python run_fft.py -i INPUT_NLL_FILE -o OUTPUT_FFT_FILE -p zscore --value norm
```

- You can specify `-p` for three normalization methods: `zscore`, `logzs`, or `minmax`.
- `--value` specifies the way for extracting spectrum data. The other two options are: `real` for real part, and `imag` for imaginary part.

The resulting spectrum data will be used as the input for the pair-wise heuristic-based classifier at **Step 3.2**. 

For example, with the following command:

```bash
python run_fft.py -i data/pubmed/pubmed_gpt-4.original.gpt2xl.nll.txt \
    -o data/pubmed/pubmed_gpt-4.original.gpt2xl.nllzs.fftnorm.txt -p zscore --value norm
```
You can obtain the spectrum of the $z$-scored likelihood in the resulting output file ended with "nllzs.fftnorm.txt".


### 3. Run classifiers 
Finally, we take the above generated likelihood or spectrum data as input and perform classification tasks.

#### 3.1 Supervised learning-based classifier

Use the script `run_sup_cls.py`, which takes as input two files `HUMAN_NLL_FILE` and `MODEL_NLL_FILE` containing the `raw` likelihood scores (from **Step 1**), and output classification results.

```bash
python run_sup_cls.py --human HUMAN_NLL_FILE --model MODEL_NLL_FILE
```

At the core of this script are two functions: `get_circular_mean` and `run_classification`.

`get_circular_mean` goes throughs the following steps:

- *Circularizes* each input raw likelihood sequence, resulting in $$N$$ sequences.
- Applies a default logarithem *z*-score transformation to each resulted sequence.
- Applies Fourier transform to  each sequence, and obtain the mean spectrum from teh N spectra.

If `--save_intermid` is set to `True`, then the mean spectrum will be saved to a text file ended with ".circlemean.txt".

`run_classification` calls `get_circular_mean` and uses its output to train and test an SVM classifier:

- The mean spectrum is processed with linear interpolation.  
- 5-fold cross validation are used for training/testing; the accuracy for each fold the final average accuracy are printed.

For example, with the following command:

```bash
python run_sup_cls.py --human data/pubmed/pubmed_gpt-4.original.gpt2xl.nll.txt \
    --model data/pubmed/pubmed_gpt-4.sampled.gpt2xl.nll.txt
    --save_intermid
```
You are expected to see the following output:

```bash
Cross-validated acc: [0.8 0.78333333 0.8 0.83333333 0.8]
Mean acc: 0.8033333333333335
```

And the intermidiate results are saved to "data/pubmed/pubmed_gpt-4.original.gpt2xl.nll.circlemean.txt" and "data/pubmed/pubmed_gpt-4.sampled.gpt2xl.nll.circlemean.txt".

**Note** that the functions of `run_sup_cls.py` overlap with `run_fft.py` in producing the spectrum data (in "*.fftnorm.txt" files). But since spectrum is just an intermidiate product for Step 3.1, we still keep the two scripts separate. In order to lessen the inter-dependency between the scripts, we encourage you to use `run_fft.py` to obtain the inputs for Step 3.2.

#### 3.2 Pairwise heuristic-based classifier

Use the script `run_pwh_cls.py`, which (similarly to `run_sup_cls.py`) takes as input two files `HUMAN_NLL_FILE` and `MODEL_NLL_FILE` containing the `raw` likelihood scores (from **Step 1**), and output classification results.:

```bash
python run_pwh_cls.py --human HUMAN_NLL_FILE --model MODEL_NLL_FILE
```

For example, with the following command:

```bash
python run_pwh_cls.py --human data/pubmed/pubmed_gpt-4.original.mistral.nllzs.fftnorm.txt --model data/pubmed/pubmed_gpt-4.sampled.mistral.nllzs.fftnorm.txt
```
which output the classification outcome:
```
Classifying a single pair of human and model spectrum data
best_k=3, best_acc=0.9000, higher=model
```
where `best_k` is the number of the first `k` frequency components (from low to high) that produces the best accuracy. `higher` indicates whether human or model has higher average power value on the selected frequency components.

The script also provides an option to fully print the classification results over all three datasets (GPT-4, GPT-3.5, and GPT-3), by solely using the `--experiments` flag:

```bash
python run_pwh_cls.py --experiments
```

which outputs the following:
```
Evaluation for gpt-4
Genre: pubmed
   pubmed, mistral, best_k=3, best_acc=0.9000, higher=model
   pubmed, gpt2xl, best_k=3, best_acc=0.9133, higher=model
   pubmed, bigram, best_k=12, best_acc=0.6533, higher=human
Genre: writing
   writing, mistral, best_k=4, best_acc=0.7667, higher=model
   writing, gpt2xl, best_k=23, best_acc=0.8467, higher=human
   writing, bigram, best_k=28, best_acc=0.8800, higher=human
Genre: xsum
   xsum, mistral, best_k=48, best_acc=0.6533, higher=human
   xsum, gpt2xl, best_k=29, best_acc=0.8733, higher=human
   xsum, bigram, best_k=34, best_acc=0.7667, higher=human

Evaluation for gpt-3.5
Genre: pubmed
...
```

---

We believe the above described scripts are sufficient to reproduce the results in our paper, but if you are interested to see more experimental details, please checkout the original Jupyter Notebook files in the [notebook]() folder:
- `circular.ipynb` conducts circularization operation on likelihood scores.
- `classifier_circlemean.ipynb` carries out supervised classification on the circularized output.
- `classifier_pairwise.ipynb` carries out heuristic-based classification.


## Thanks
We sincerely thank [Guangsheng Bao](https://baoguangsheng.github.io/), the author of [Fast-DetectGPT](https://arxiv.org/abs/2310.05130), for the helpful discussions and for sharing the data.

## Notes and citation
This repository is currently under construction. Additional data and code will be added soon.

If you find our work useful, you can cite it with:
```bibtex
@article{xu2024detecting,
    title={Detecting Subtle Differences between Human and Model Languages Using Spectrum of Relative Likelihood},
    author={Yang Xu and Yu Wang and Hao An and Zhichen Liu and Yongyuan Li},
    journal={arXiv preprint arXiv:2406.19874},
    year={2024}
}
```
